---
import BaseLayout from "../layouts/BaseLayout.astro";
import HorizontalCard from "../components/HorizontalCard.astro";
---

<BaseLayout title = "Projects" sideBarActiveItemID="projects">
  <div>
    <div class="text-3xl w-full font-bold mb-5">Projects Header</div>
  </div>

  <HorizontalCard
    title="STC, Bahrain"
    img="/post_img.webp"
    desc="Designed and executed solution architectures for Cloudera environments, leading the procurement and implementation of new hardware and software to enhance system capabilities. I also designed and developed ETL pipelines using Apache Spark and NiFi to support data ingestion and processing into the Data Lake. I was responsible for modifications and deployment of these pipelines from the QA environment to production (PROD). To strengthen security and simplify authorization, I integrated Cloudera with Kerberos using Active Directory. I installed and configured Cloudera services and ETL tools, ensuring optimal setup and performance across the platform. Additionally, I implemented and upgraded SSL certificates on all production nodes to enhance data security. My efforts included optimizing Kudu table performance and balancing HDFS and Kudu data loads to reduce latency and improve data processing efficiency. I also led the installation and integration of Cloudera NiFi, replacing Streamsets to streamline ETL processes and enhance data ingestion workflows."
    url="#"
    badge="NEW"
  />
  <div class="divider my-0"></div>
  <HorizontalCard
    title="KAFD, Saudi Arabia"
    img="/post_img.webp"
    desc="Installed Cloudera environment (Cloudera 7.1.9 with ECS 5.1.2) for the client. Solution involved installing Cloudera’s Embedded Containerized Service which uses Kubernetes underneath for Data Engineering, Data Warehousing and Machine Learning. Provided Hardware Specs for the environment."
    url="#"
  />

  <div class="divider my-0"></div>
  
  <div>
    <div class="text-3xl w-full font-bold mb-5 mt-10">Projects Header</div>
  </div>

  <HorizontalCard
    title="Allied Bank Limited, Pakistan"
    img="/post_img.webp"
    desc="Developed and integrated a unified Customer 360 model, consolidating data from multiple source systems to generate key performance indicators (KPIs) aligned with business requirements. As part of a strategic initiative, I planned and implemented a data migration activity from the existing Oracle 19c CDB Plus warehouse to the Cloudera CDH 5.x-based Data Lake. I was responsible for user acceptance testing (UAT) of the data migrated from CDB to the Data Lake, ensuring accuracy and completeness. In addition, I led a team of highly technical professionals responsible for the development and maintenance of both production and disaster recovery (DR) sites for the client's Data Lake infrastructure. I modified Cloudera configurations to optimize service performance and job execution, including service-level tuning and YARN queue management. Furthermore, I assisted the client in migrating data and ETL logic for over 3,000 tables, stored procedures, and views to the Big Data Appliance (BDA), the current Data Lake platform. This migration included ingesting data from core systems such as T24 Temenos (core banking), as well as other key systems like Credit Card, IRIS, ATM, and more."
    url="#"
  />

  <div class="divider my-0"></div>
  <HorizontalCard
    title="BAJ, Saudi Arabia"
    img="/post_img.webp"
    desc="Assisted in providing HW specifications for Cloudera Datalake Solution. Installed and maintained Cloudera environment from scratch on BAJ Project. Installed Cloudera 7.1.8 with ECS 5.1.2."
    url="#"
  />

  <div class="divider my-0"></div>
  <HorizontalCard
    title="SAIB, Saudi Arabia"
    img="/post_img.webp"
    desc="Installed and configured Denodo for various use cases, integrating with the client’s source systems and Cloudera Data Lake to support data virtualization needs."
    url="#"
  />

   <div class="divider my-0"></div>
  <HorizontalCard
    title="Altron"
    img="/post_img.webp"
    desc="Designed an end-to-end solution for data ingestion, processing, and storage on Google Cloud Platform (GCP) for the client. As part of this solution, I developed a data warehouse using Cloud SQL to handle structured data storage. I also designed and built ETL pipelines leveraging Cloud Functions, Compute Engine, and Cloud Scheduler, with Python and MySQL scripts playing a central role in the data transformation process. To ensure seamless integration, I implemented logic to provide transformed data to downstream applications. One of the key achievements was the complete removal of dependency on the client’s previous system, which relied on STATA for data transformation and stored data in CSV format on OneDrive. I successfully completed user acceptance testing (UAT) and carried out necessary modifications for the deployed pipelines and queries on GCP. Additionally, I remained responsible for handling all client-requested service-level and code-level changes."
    url="#"
  />
   <div class="divider my-0"></div>
  <HorizontalCard
    title="Bank Al Falah, Pakistan"
    img="/post_img.webp"
    desc="Developed and implemented real-time data ingestion pipelines using Kafka and Spark Streaming, enabling seamless data flow and supporting real-time analytics for critical business needs. I managed the deployment and ongoing maintenance of HDP 3 (Hadoop Data Platform) clusters, which included setting up and optimizing disaster recovery (DR) environments to ensure high availability and resilience. In addition to real-time processing, I designed and built robust ETL pipelines for both batch and streaming data ingestion using tools like Apache Spark and Kafka. I also actively led a team of highly motivated and skilled professionals responsible for operating and maintaining a data lake architecture built on top of Hadoop, ensuring its stability, scalability, and performance."
    url="#"
  />
</BaseLayout>
